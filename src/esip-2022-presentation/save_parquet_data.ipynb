{"metadata": {"language_info": {"name": "python", "version": "3.9.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"name": "python3", "display_name": "Python 3 (ipykernel)", "language": "python"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "code", "source": "from dask import dataframe as dd\nfrom dask.distributed import Client\nfrom dask_gateway import Gateway\n\nimport pandas as pd\nimport xarray as xr\nimport fsspec\nimport s3fs", "metadata": {}, "execution_count": 1, "outputs": [], "id": "f8722aea-188c-4a17-aaad-a8a0bb5f09c2"}, {"cell_type": "code", "source": "# Connect to existing cluster using cluster.name\n\n# This constant needs to be set!\ncluster_name = ''\ngateway = Gateway()\ncluster = gateway.connect(cluster_name)\nclient = cluster.get_client()", "metadata": {}, "execution_count": null, "outputs": [], "id": "77055782-c06d-4d82-9b65-b9e286327a78"}, {"cell_type": "code", "source": "comid_uri = 's3://azavea-noaa-hydro-data/noaa/huc2-comids.json'\nzarr_uri = 's3://azavea-noaa-hydro-data/esip-experiments/datasets/reanalysis-chrtout/zarr/lf/07-07-2022c/nwm-subset.zarr'\nout_root_uri = 's3://azavea-noaa-hydro-data/esip-experiments/datasets/reanalysis-chrtout/parquet/lf/07-13-2022a'\nparq_uri = join(out_root_uri, 'nwm-subset')\n\nds = xr.open_zarr(fsspec.get_mapper(zarr_subsets_uri))\nds", "metadata": {}, "execution_count": null, "outputs": [], "id": "86c2630a-cd92-4973-8d77-c8357f1652a3"}, {"cell_type": "code", "source": "def part_namer(yrmon):\n    \"\"\"\n    >>> fns = [part_namer(i) for i in range(1990,1995)]\n    >>> [fn(i) for i in range(3) for fn in fns]\n    \"\"\"\n    if isinstance(yrmon, str):\n        yrmon = int(yrmon)\n        \n    def fn(idx):\n        return f\"{yrmon:06}-part.{idx:02}.parquet\"\n    \n    return fn", "metadata": {}, "execution_count": 14, "outputs": [], "id": "d7c686ef-c50e-4dd9-92a6-09de06c6100c"}, {"cell_type": "code", "source": "years = [1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999]\nfor yr in years:\n      df01 = ds.sel(time=f'{yr}-01').to_dask_dataframe(); dd.to_parquet(df01, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}01'), write_metadata_file=True); del df01; print(f'wrote {yr}-01')\n      df02 = ds.sel(time=f'{yr}-02').to_dask_dataframe(); dd.to_parquet(df02, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}02'), write_metadata_file=True); del df02; print(f'wrote {yr}-02')\n      df03 = ds.sel(time=f'{yr}-03').to_dask_dataframe(); dd.to_parquet(df03, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}03'), write_metadata_file=True); del df03; print(f'wrote {yr}-03')\n      df04 = ds.sel(time=f'{yr}-04').to_dask_dataframe(); dd.to_parquet(df04, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}04'), write_metadata_file=True); del df04; print(f'wrote {yr}-04')\n      df05 = ds.sel(time=f'{yr}-05').to_dask_dataframe(); dd.to_parquet(df05, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}05'), write_metadata_file=True); del df05; print(f'wrote {yr}-05')\n      df06 = ds.sel(time=f'{yr}-06').to_dask_dataframe(); dd.to_parquet(df06, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}06'), write_metadata_file=True); del df06; print(f'wrote {yr}-06')\n      df07 = ds.sel(time=f'{yr}-07').to_dask_dataframe(); dd.to_parquet(df07, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}07'), write_metadata_file=True); del df07; print(f'wrote {yr}-07')\n      df08 = ds.sel(time=f'{yr}-08').to_dask_dataframe(); dd.to_parquet(df08, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}08'), write_metadata_file=True); del df08; print(f'wrote {yr}-08')\n      df09 = ds.sel(time=f'{yr}-09').to_dask_dataframe(); dd.to_parquet(df09, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}09'), write_metadata_file=True); del df09; print(f'wrote {yr}-09')\n      df10 = ds.sel(time=f'{yr}-10').to_dask_dataframe(); dd.to_parquet(df10, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}10'), write_metadata_file=True); del df10; print(f'wrote {yr}-10')\n      df11 = ds.sel(time=f'{yr}-11').to_dask_dataframe(); dd.to_parquet(df11, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}11'), write_metadata_file=True); del df11; print(f'wrote {yr}-11')\n      df12 = ds.sel(time=f'{yr}-12').to_dask_dataframe(); dd.to_parquet(df12, ds_path, engine='pyarrow', name_function=part_namer(f'{yr}12'), write_metadata_file=True); del df12; print(f'wrote {yr}-12')\n\n## ds had 2000-01-01!\ndf = ds.sel(time='2000-01').to_dask_dataframe()", "metadata": {}, "execution_count": null, "outputs": [], "id": "5b21ef16-7e66-418d-8e79-625459fc0cf2"}, {"cell_type": "code", "source": "dd.to_parquet(df, ds_path, engine='pyarrow', name_function=part_namer('200001'), write_metadata_file=True)\ndel df", "metadata": {}, "execution_count": 19, "outputs": [], "id": "6d3e757f-8c21-4cf6-9ef0-be79d07e90e4"}]}