#!/usr/bin/env python
# coding: utf-8

# # Benchmark HUC8 Streamflow Aggregation Queries
# 
# This notebook show how to query the NWM reanalysis dataset using HUC8s in various ways, time the queries, and save the results in a CSV.

# ## Setup

import json
from os.path import join
import time

from tqdm.notebook import tqdm
import geopandas as gpd
import xarray as xr
import fsspec
import numpy as np
import pyproj
from dask.distributed import Client
from dask_gateway import Gateway
import dask.dataframe as dd
import numpy as np
import pandas as pd
import fsspec

get_ipython().run_line_magic('matplotlib', 'inline')

def get_json(uri):
    with fsspec.open(uri) as fd:
        return json.load(fd)


# Connect to existing cluster using cluster.name

# This constant needs to be set!
cluster_name = ''
gateway = Gateway()
cluster = gateway.connect(cluster_name)
client = cluster.get_client()


# Get COMIDs for a HUC8 around Philly from a HUC8 extract on S3.
# Each COMID represents a stream reach.

# Location of HUC8 extract JSON files.
huc8_root_uri = 's3://azavea-noaa-hydro-data/noaa/huc8-extracts/transformed/'

# TODO: run this with multiple HUC8s.
philly_huc8 = '02040202'
huc8_uri = join(huc8_root_uri, f'{philly_huc8}.json')

huc8_dict = get_json(huc8_uri)
comids = huc8_dict['features'][0]['properties']['comids']

zarr_uri = 's3://azavea-noaa-hydro-data/esip-experiments/datasets/reanalysis-chrtout/zarr/lf/07-07-2022c/nwm-subset.zarr'
ds = xr.open_zarr(zarr_uri)

# Apparently, only some of the reach ids in NHDPlus V2 are available in NWM.
# Question: why is that?
avail_comids = list(set(ds.feature_id.values).intersection(set(comids)))

# Need to sort or you will get warnings about a slowdown with an out of order index.
avail_comids.sort()
print(
    f'There are {len(comids)} reaches in the HUC and {len(avail_comids)} of those are in NWM.')

del ds


# ## Run timing experiments

# Set things commons across all experiments.

# The number of times to repeat execution
nb_repeats = 5
time_ranges = [
    slice('1990-01-01', '2000-01-01'), 
    slice('1990-01-01', '1991-01-01'),
    slice('1990-01-01', '1990-02-01'),
]
nb_reaches = len(avail_comids)
nb_workers = len(client.scheduler_info()['workers'])


# ### Zarr experiments

# Settings

# A map from query nicknames to functions that execute the query.
query_map = {
    'mean_features_mean_day': (lambda ds: ds.streamflow.mean(dim='feature_id').groupby('time.dayofyear').mean().values),
    'mean_day': (lambda ds: ds.streamflow.groupby('time.dayofyear').mean().values),
    'mean_week': (lambda ds: ds.streamflow.groupby('time.weekofyear').mean().values)
}
data_format = 'zarr'

# The CHRTOUT data from the NWM Retrospective Zarr 2.1 dataset
# This has "Streamflow values at points associated with flow lines" 
# See https://registry.opendata.aws/nwm-archive/
# Original dataset is at s3://noaa-nwm-retrospective-2-1-zarr-pds/chrtout.zarr

# 10 year subset and transposed chunk version generated by save_zarr_data.ipynb
zarr_orig_uri = 's3://azavea-noaa-hydro-data/esip-experiments/datasets/reanalysis-chrtout/zarr/lf/07-07-2022c/nwm-subset.zarr'
zarr_trans_uri = 's3://azavea-noaa-hydro-data/esip-experiments/datasets/reanalysis-chrtout/zarr/lf/07-07-2022c/trans-chunk.zarr'
zarr_uris = [zarr_orig_uri, zarr_trans_uri]

# Set this to a new URI!
zarr_results_uri = ''


# Run this block to use shorter test settings.
query_map = {
    'mean_day': query_map['mean_day']}
zarr_uris = zarr_uris[0:1]
nb_repeats = 3


get_ipython().run_cell_magic('time', '', "\nzarr_exp_rows = []\nfor zarr_uri in tqdm(zarr_uris, desc='zarr stores', leave=False):\n    ds = xr.open_zarr(zarr_uri)\n    for time_range in tqdm(time_ranges, desc='time ranges', leave=False):\n        sub_ds = ds.sel(feature_id=avail_comids, time=time_range)\n\n        nb_days = (pd.to_datetime(time_range.stop) - pd.to_datetime(time_range.start)).days\n        \n        chunk_sizes = np.array([ds.streamflow.chunks[0][0], ds.streamflow.chunks[1][0]])\n        time_chunk_sz = chunk_sizes[0]\n        feature_id_chunk_sz = chunk_sizes[1]\n        \n        for qname, qfunc in tqdm(query_map.items(), desc='query', leave=False):\n            for repeat_ind in tqdm(range(nb_repeats), desc='repeat', leave=False):\n                start_time = time.time()\n                vals = qfunc(sub_ds)\n                elapsed = time.time() - start_time\n                exp_row = {\n                    'query': qname,\n                    'time': elapsed,\n                    'nb_reaches': nb_reaches,\n                    'nb_days': nb_days,\n                    'data_format': data_format,\n                    'time_chunk_sz': time_chunk_sz,\n                    'feature_id_chunk_sz': feature_id_chunk_sz, \n                    'nb_workers': nb_workers,\n                    'repeat_ind': repeat_ind\n                }\n                zarr_exp_rows.append(exp_row)\n        del sub_ds\n    del ds\n")


exp_rows = zarr_exp_rows
df = pd.DataFrame(exp_rows)
df.to_csv(zarr_results_uri)
df


# ### Parquet experiments

# Settings

# A map from query nicknames to functions that execute the query.
query_map = {
    'mean_day': (lambda df: df.groupby(df.index.dayofyear).streamflow.mean().values.compute()),
    'mean_week': (lambda df: df.groupby(df.index.weekofyear).streamflow.mean().values.compute()),
    'mean_features_mean_day': (lambda df: df.groupby(['feature_id',df.index.dayofyear]).streamflow.mean().values.compute()),
}
data_format = 'parquet'

# 10 years with metadata
parq_uri = 's3://azavea-noaa-hydro-data/esip-experiments/datasets/reanalysis-chrtout/parquet/vl/07-13-2022c_metadata/'
parq_uris = [parq_uri]

parq_results_uri = ''


get_ipython().run_cell_magic('time', '', "\nparq_exp_rows = []\nfor parq_uri in tqdm(parq_uris, desc='parqet stores', leave=False):\n    df = dd.read_parquet(parq_uri, engine='pyarrow', index='time', columns=['feature_id','streamflow'])\n    for time_range in tqdm(time_ranges, desc='time ranges', leave=False):\n        sub_df = df.query(\n            'feature_id in @avail_comids and time > @start_time and time < @end_time', \n            local_dict={\n                'avail_comids': avail_comids, \n                'start_time': time_range.start,\n                'end_time': time_range.stop})\n\n        nb_days = (pd.to_datetime(time_range.stop) - pd.to_datetime(time_range.start)).days\n        \n        # TODO: make the next lines of code valid\n        time_chunk_sz = -9999\n        feature_id_chunk_sz = -9999\n        \n        for qname, qfunc in tqdm(query_map.items(), desc='query', leave=False):\n            for repeat_ind in tqdm(range(nb_repeats), desc='repeat', leave=False):\n                start_time = time.time()\n                vals = qfunc(sub_df)\n                elapsed = time.time() - start_time\n                exp_row = {\n                    'query': qname,\n                    'time': elapsed,\n                    'nb_reaches': nb_reaches,\n                    'nb_days': nb_days,\n                    'data_format': data_format,\n                    'time_chunk_sz': time_chunk_sz,\n                    'feature_id_chunk_sz': feature_id_chunk_sz,\n                    'repeat_ind': repeat_ind\n                }\n                parq_exp_rows.append(exp_row)\n        del sub_df\n    del df\n")


## df = pd.DataFrame(parq_exp_rows)
## df.to_csv(parq_results_uri)
## df


parq_results_uri = 's3://azavea-noaa-hydro-data/esip-experiments/benchmarks/vl/07-20-2022c.csv'
results = pd.DataFrame(parq_exp_rows)
results.to_csv(parq_results_uri)
results




