#!/usr/bin/env python
# coding: utf-8

# # Plot results

# ## Setup

# seaborn is not installed in notebook Docker image
get_ipython().system(' pip install seaborn')


from os.path import join
import tempfile

import pandas as pd
import seaborn as sns


# This code was adapted from Raster Vision
# https://github.com/azavea/raster-vision/blob/master/rastervision_aws_s3/rastervision/aws_s3/s3_file_system.py
import os
from urllib.parse import urlparse

import boto3

s3 = boto3.client('s3')

def parse_uri(uri: str):
    """Parse bucket name and key from an S3 URI."""
    parsed_uri = urlparse(uri)
    bucket, key = parsed_uri.netloc, parsed_uri.path[1:]
    return bucket, key

def copy_to_s3(src_path: str, dst_uri: str) -> None:
    bucket, key = parse_uri(dst_uri)
    file_size = os.path.getsize(src_path)
    try:
        s3.upload_file(
            Filename=src_path,
            Bucket=bucket,
            Key=key)
    except Exception as e:
        raise NotWritableError(f'Could not write {dst_uri}') from e


out_root_uri = 's3://azavea-noaa-hydro-data/esip-experiments/plots/zarr/lf/07-14-2022a/'

# Read in results generated by benchmark_queries.ipynb.
results_uri = 's3://azavea-noaa-hydro-data/esip-experiments/benchmarks/zarr/lf/07-13-2022a.csv'
df = pd.read_csv(results_uri)
df


# Add chunk_sz string column for convenience.
df['chunk_sz'] = [f'({x}, {y})' for x, y in zip(df.time_chunk_sz.values, df.feature_id_chunk_sz.values)]

# Add human readable column names for plotting purposes. It would be nice if we didn't have to do this, and could 
# just tell the plotting function how to translate column names.
df['chunk shape'] = df['chunk_sz']
df['run time: secs'] = df['time_mean']
df['days in query'] = df['nb_days']
df.head()


# ## Plot using `seaborn`

# ### Make bar plot that is a visual dump of the whole dataframe.
# 
# Plotting concepts:
# * things you can control in plot: x, y, hue, row, col, 
# * independent vars: query, nb_days, chunk_sz
# * (other independent vars we aren't varying yet): data_format, nb_reaches, nb_workers
# * dependent vars: time_mean, time_std
# 
# Observations:
# * The transposed chunk size (30000, 672) is much faster for large queries over 3652 days, but doesn't have much of an effect for smaller queries.
# * The different queries all take roughly the same amount of time.

plot = sns.catplot(
    x='days in query', y='run time: secs', col='query', hue='chunk shape', 
    kind='bar', data=df)
fig = plot.fig
fig.subplots_adjust(top=0.85)
fig.suptitle('Timing streamflow queries using Zarr')


# Save plot to S3. 
plot_uri = join(out_root_uri, 'plot.png')
# All this rigmarole is needed because savefig only takes a string to a local path.
with tempfile.NamedTemporaryFile() as tf:
    fig.savefig(tf.name, format='png', dpi=100)
    copy_to_s3(tf.name, plot_uri)


# ## Plot using `pandas`
# 
# (This isn't as good as using seaborn, but is here just in case).

# This groups by chunk size over all experiments to see the effect of this indpendent variable.
_df = df.groupby('chunk_sz', as_index=False).mean()
_df


_df.plot('chunk_sz', 'time_mean', 'bar')

